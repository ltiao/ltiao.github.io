<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Louis Tiao (algebra)</title><link>http://louistiao.me/</link><description></description><atom:link href="http://louistiao.me/tags/algebra.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Wed, 28 Oct 2015 11:35:51 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C. M. 2007)</title><link>http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007.html</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;div class="section" id="equation-3-13"&gt;
&lt;h2&gt;Equation 3.13&lt;/h2&gt;
&lt;p&gt;The gradient of the log-likelihood function is given in equation 3.13:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)^T
\end{equation*}
&lt;/div&gt;
&lt;p&gt;We derive this step-by-step below. First, the log-likelihood function is given
by equation 3.11 and reproduced below,&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\ln p(\mathbf{t} \mid \mathbf{w}, \beta)
&amp;amp;= \sum_{n=1}^N \ln \mathcal{N}(t_n \mid \mathbf{w}^T \phi(\mathbf{x}_n), \beta^{-1}) \\
&amp;amp;= \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2 \pi - \beta E_D(\mathbf{w})
\end{align*}
&lt;/div&gt;
&lt;p&gt;where the sum-of-squares error function is given by equation 3.12,&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Then &lt;span class="math"&gt;\(\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)\)&lt;/span&gt;, the
gradient of the log-likelihood function &lt;em&gt;with respect to&lt;/em&gt; &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; &lt;a class="footnote-reference" href="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007.html#id2" id="id1"&gt;[1]&lt;/a&gt;
is given by,&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta) = - \beta \nabla_{\mathbf{w}} E_D(\mathbf{w})
\end{equation*}
&lt;/div&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} E_D
= \begin{bmatrix}
    \frac{\partial E_D}{\partial w_0} \\
    \vdots \\
    \frac{\partial E_D}{\partial w_{M-1}}
  \end{bmatrix}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{\partial E_D}{\partial w_k}
&amp;amp;= \frac{\partial}{\partial w_k} \frac{1}{2} \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 \\
&amp;amp;= \frac{1}{2} \sum_{n=1}^N \frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 \\
\end{align*}
&lt;/div&gt;
&lt;p&gt;Since&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2
= 2 \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}
\end{equation*}
&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}
&amp;amp;= - \frac{\partial}{\partial w_k} \mathbf{w}^T \phi(\mathbf{x}_n) \\
&amp;amp;= - \frac{\partial}{\partial w_k} \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}_n) \\
&amp;amp;= - \phi_k(\mathbf{x}_n)
\end{align*}
&lt;/div&gt;
&lt;p&gt;we have&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2
= - 2 \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi_k(\mathbf{x}_n)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;and therefore&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\frac{\partial E_D}{\partial w_k}
&amp;amp;= \frac{1}{2} \sum_{n=1}^N \frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 \\
&amp;amp;= - \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi_k(\mathbf{x}_n) \\
\end{align*}
&lt;/div&gt;
&lt;p&gt;Finally, we have&lt;/p&gt;
&lt;div class="math"&gt;
\begin{align*}
\nabla_{\mathbf{w}} E_D
&amp;amp;= \begin{bmatrix}
     \frac{\partial E_D}{\partial w_0} \\
     \vdots \\
     \frac{\partial E_D}{\partial w_{M-1}}
   \end{bmatrix} \\
&amp;amp;= - \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}
   \begin{bmatrix}
     \phi_0(\mathbf{x}_n) \\
     \vdots \\
     \phi_{M-1}(\mathbf{x}_n)
   \end{bmatrix} \\
&amp;amp;= - \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)
\end{align*}
&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)
\end{equation*}
&lt;/div&gt;
&lt;p&gt;Note that the final &lt;span class="math"&gt;\(\phi(\mathbf{x}_n)\)&lt;/span&gt; in equation 3.13 is given as
its transpose, so that &lt;span class="math"&gt;\(\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)\)&lt;/span&gt;
is a row vector. This was probably done to ease the derivation of the analytic
solution which following immediately in the section. The only thing we need to
alter in our derivation to make the result consistent with that of the book is
to rewrite &lt;span class="math"&gt;\(\nabla_{\mathbf{w}} E_D\)&lt;/span&gt; as a row vector,&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} E_D
= \begin{bmatrix}
    \frac{\partial E_D}{\partial w_0} &amp;amp; \cdots &amp;amp; \frac{\partial E_D}{\partial w_{M-1}}
  \end{bmatrix}
\end{equation*}
&lt;/div&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007.html#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Since there is little possibility of confusion, the book omits the
subscript and simply writes &lt;span class="math"&gt;\(\nabla\)&lt;/span&gt; to denote the gradient w.r.t.
&lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt;.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="equation-3-21"&gt;
&lt;h2&gt;Equation 3.21&lt;/h2&gt;
&lt;div class="math"&gt;
\begin{align*}
\ln p(\mathbf{t} \mid \mathbf{w}, \beta)
&amp;amp;= \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2 \pi - \beta E_D(\mathbf{w}) \\
\frac{\partial}{\partial \beta} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
&amp;amp;= \frac{N}{2 \beta} - E_D(\mathbf{w})
\end{align*}
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>algebra</category><category>gradients</category><category>linear algebra</category><category>machine learning</category><category>mathjax</category><guid>http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007.html</guid><pubDate>Wed, 28 Oct 2015 05:31:44 GMT</pubDate></item></channel></rss>