<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Louis Tiao (Posts about variational inference)</title><link>http://louistiao.me/</link><description></description><atom:link href="http://louistiao.me/tags/variational-inference.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2017 &lt;a href="mailto:louistiao@me.com"&gt;Louis Tiao&lt;/a&gt; </copyright><lastBuildDate>Mon, 20 Nov 2017 15:04:05 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Inference in Variational Autoencoders with Different Monte Carlo Sample Sizes</title><link>http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;div class="admonition admonition-draft"&gt;
&lt;p class="first admonition-title"&gt;Draft&lt;/p&gt;
&lt;p class="last"&gt;Please do not share or link.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In a &lt;a class="reference external" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"&gt;previous post&lt;/a&gt;,
I demonstrated how to use leverage Keras' modular design to implement variational
inference in a way that makes it easy to tweak hyperparameters, adapt to related
models, and extend to the more sophisticated methods in the current research.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_988102aa0bb64b0998a8effa4bcc7c11-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;eps&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mc_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;latent_dim&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Everything else remains exactly the same. The &lt;tt class="docutils literal"&gt;Multiply&lt;/tt&gt; layer will
automatically broadcast &lt;tt class="docutils literal"&gt;eps&lt;/tt&gt; which has shape
&lt;tt class="docutils literal"&gt;(batch_size, mc_samples, latent_dim)&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;sigma&lt;/tt&gt; which has shape
&lt;tt class="docutils literal"&gt;(batch_size, latent_dim)&lt;/tt&gt; and output shape
&lt;tt class="docutils literal"&gt;(batch_size, mc_samples, latent_dim)&lt;/tt&gt;. Since the subsequent layers do not
operate on the which will then be propagated to the
final output.&lt;/p&gt;
&lt;p&gt;diagram here&lt;/p&gt;
&lt;p&gt;We expand the targets to 3d a array &lt;tt class="docutils literal"&gt;np.expand_dims(x_train, axis=1)&lt;/tt&gt; to be
of shape &lt;tt class="docutils literal"&gt;(batch_size, 1, original_dim)&lt;/tt&gt; so that the loss function can
broadcast with the output with shape &lt;tt class="docutils literal"&gt;(batch_size, mc_samples, original_dim)&lt;/tt&gt;.
It is important to make the distinction between the log likelihood of the mean
over outputs, versus the mean of the log likelihood over the outputs. Since we
require the expected log likelihood, we are interested in the latter.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;eps_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;mc_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;latent_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;eps_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;mc_samples&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;latent_dim&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-3"&gt;&lt;/a&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;vae&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-5"&gt;&lt;/a&gt;    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps_train&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-6"&gt;&lt;/a&gt;    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-7"&gt;&lt;/a&gt;    &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-8"&gt;&lt;/a&gt;    &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-9"&gt;&lt;/a&gt;    &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-10"&gt;&lt;/a&gt;    &lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-11"&gt;&lt;/a&gt;        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;eps_test&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-12"&gt;&lt;/a&gt;        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expand_dims&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-13"&gt;&lt;/a&gt;    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_b94ce429bede421c8de9c48c6a4267ad-14"&gt;&lt;/a&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;For every data point, there are &lt;tt class="docutils literal"&gt;mc_samples&lt;/tt&gt; reconstructions.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_348a451139a047eab1a100bb6d7c7fa6-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;recons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vae&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x_test&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;eps_test&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_348a451139a047eab1a100bb6d7c7fa6-2"&gt;&lt;/a&gt;
&lt;a name="rest_code_348a451139a047eab1a100bb6d7c7fa6-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;a name="rest_code_348a451139a047eab1a100bb6d7c7fa6-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;block&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recons&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)))),&lt;/span&gt;
&lt;a name="rest_code_348a451139a047eab1a100bb6d7c7fa6-5"&gt;&lt;/a&gt;           &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_348a451139a047eab1a100bb6d7c7fa6-6"&gt;&lt;/a&gt;&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;plot here&lt;/p&gt;&lt;/div&gt;</description><category>bayesian</category><category>deep learning</category><category>keras</category><category>mathjax</category><category>python</category><category>representation learning</category><category>tensorflow</category><category>unsupervised learning</category><category>variational autoencoder</category><category>variational inference</category><guid>http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/</guid><pubDate>Mon, 20 Nov 2017 12:51:24 GMT</pubDate></item><item><title>Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial</title><link>http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;div class="admonition admonition-draft"&gt;
&lt;p class="first admonition-title"&gt;Draft&lt;/p&gt;
&lt;p class="last"&gt;Please do not share or link.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a class="reference external" href="https://keras.io/"&gt;Keras&lt;/a&gt; is awesome. It is a very well-designed library that clearly abides by to
its &lt;a class="reference external" href="https://keras.io/#guiding-principles"&gt;guiding principles&lt;/a&gt; of modularity and extensibility and thereby allows us
to easily assemble powerful complex models from primitive building blocks.
This has been demonstrated by many blog posts and tutorials, such as the
excellent tutorial on &lt;a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html"&gt;Building Autoencoders in Keras&lt;/a&gt;.
As the name suggests, that tutorial provides examples of how to implement
various kinds of autoencoders in Keras, including the variational autoencoder
(VAE) &lt;a class="footnote-reference" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/#kingma2014" id="id1"&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;img alt="../../images/vae/result_combined.png" src="http://louistiao.me/images/vae/result_combined.png"&gt;
&lt;p class="caption"&gt;Visualization of 2D manifold of MNIST digits (left)
and the representation of digits in latent space colored according to their
digit labels (right).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Like all autoencoders, the variational autoencoder are primarily used for
unsupervised learning of hidden representations.
However, variational autoencoders are fundamentally different to your standard
neural network-based autoencoder in that they tackle the problem with a
probabilistic approach: by specifying distributions over the observed and
latent variables, and approximating the intractable posterior over the latter
using variational inference with an &lt;em&gt;inference network&lt;/em&gt;
&lt;a class="footnote-reference" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/#inference1" id="id2"&gt;[2]&lt;/a&gt; &lt;a class="footnote-reference" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/#inference2" id="id3"&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"&gt;Read moreâ¦&lt;/a&gt; (14 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>deep learning</category><category>keras</category><category>mathjax</category><category>python</category><category>representation learning</category><category>tensorflow</category><category>unsupervised learning</category><category>variational autoencoder</category><category>variational inference</category><guid>http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/</guid><pubDate>Sun, 22 Oct 2017 14:19:59 GMT</pubDate></item></channel></rss>