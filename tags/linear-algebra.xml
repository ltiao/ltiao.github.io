<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Louis Tiao (linear algebra)</title><link>http://louistiao.me/</link><description></description><atom:link href="http://louistiao.me/tags/linear-algebra.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Mon, 04 Jan 2016 11:38:20 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C. M. 2007)</title><link>http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div class="section" id="equation-3-13"&gt;
&lt;h2&gt;Equation 3.13&lt;/h2&gt;
&lt;p&gt;The gradient of the log-likelihood function is given in equation 3.13:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)^T
\end{equation*}
&lt;/div&gt;
&lt;p&gt;We derive this step-by-step below. First, the log-likelihood function is given
by equation 3.11 and reproduced below,&lt;/p&gt;
&lt;p&gt;&lt;a href="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/"&gt;Read moreâ€¦&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>algebra</category><category>gradients</category><category>linear algebra</category><category>machine learning</category><category>mathjax</category><guid>http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/</guid><pubDate>Wed, 28 Oct 2015 05:31:44 GMT</pubDate></item></channel></rss>