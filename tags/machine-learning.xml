<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Louis Tiao (machine learning)</title><link>http://louistiao.me/</link><description></description><atom:link href="http://louistiao.me/tags/machine-learning.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Mon, 04 Jan 2016 11:38:20 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C. M. 2007)</title><link>http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div class="section" id="equation-3-13"&gt;
&lt;h2&gt;Equation 3.13&lt;/h2&gt;
&lt;p&gt;The gradient of the log-likelihood function is given in equation 3.13:&lt;/p&gt;
&lt;div class="math"&gt;
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)^T
\end{equation*}
&lt;/div&gt;
&lt;p&gt;We derive this step-by-step below. First, the log-likelihood function is given
by equation 3.11 and reproduced below,&lt;/p&gt;
&lt;p&gt;&lt;a href="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/"&gt;Read moreâ€¦&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>algebra</category><category>gradients</category><category>linear algebra</category><category>machine learning</category><category>mathjax</category><guid>http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/</guid><pubDate>Wed, 28 Oct 2015 05:31:44 GMT</pubDate></item><item><title>Serious shortcomings of n-gram feature spaces in text classification</title><link>http://louistiao.me/posts/serious-shortcomings-of-n-gram-feature-spaces-in-text-classification/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;p&gt;The major drawback of feature spaces represented by &lt;span class="math"&gt;\(n\)&lt;/span&gt;-gram models is
extreme sparcity.&lt;/p&gt;
&lt;p&gt;But even more unsettling is that it can only interpret unseen instances with
respect to learned training data. That is, if a classifier learned from the
instances &lt;em&gt;'today was a good day'&lt;/em&gt; and &lt;em&gt;'that is a ridiculous thing to say'&lt;/em&gt;,
it is unable to say much about the instance &lt;em&gt;'i love this song!'&lt;/em&gt; since the
features are &lt;em&gt;'today', 'was', 'a', 'good', 'day', 'that', 'is', 'ridiculous',
'thing', 'to', 'say'&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;It is impossible to classify this new instance because it is entirely
meaningless to the classifier - it cannot be represented. So no matter how
many millions of instances the classifier learns from, by knowing the feature
space, one can always artificially construct "hard" examples by using words
not in the feature space.&lt;/p&gt;
&lt;p&gt;So we see this model is only well-suited for extremely large amounts of
training data &lt;a class="footnote-reference" href="http://louistiao.me/posts/serious-shortcomings-of-n-gram-feature-spaces-in-text-classification/#id2" id="id1"&gt;[1]&lt;/a&gt; - but even then, there is no guarantee that it is able to
represent &lt;em&gt;all&lt;/em&gt; unseen instances in its feature space.&lt;/p&gt;
&lt;p&gt;The Iris flower data set is a very typical test case for many statistical
classification techniques. An interesting observation is that for an English
sentence to be valid, it need not &lt;em&gt;necessarily&lt;/em&gt; contain specific words, like
&lt;em&gt;'was'&lt;/em&gt; or &lt;em&gt;'good'&lt;/em&gt; for example. Yet, for an iris flower to &lt;em&gt;be an iris
flower&lt;/em&gt;, it necessarily has sepals and petals with their respective widths and
lengths.&lt;/p&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="http://louistiao.me/posts/serious-shortcomings-of-n-gram-feature-spaces-in-text-classification/#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;Halevy, Alon, Peter Norvig, and Fernando Pereira. "The unreasonable
effectiveness of data." Intelligent Systems, IEEE 24.2 (2009): 8-12.&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;</description><category>classification</category><category>machine learning</category><category>mathjax</category><category>n-gram</category><category>natural language processing</category><guid>http://louistiao.me/posts/serious-shortcomings-of-n-gram-feature-spaces-in-text-classification/</guid><pubDate>Wed, 15 Jan 2014 00:44:00 GMT</pubDate></item></channel></rss>