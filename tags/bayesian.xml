<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Louis Tiao (Posts about bayesian)</title><link>http://louistiao.me/</link><description></description><atom:link href="http://louistiao.me/tags/bayesian.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:louistiao@me.com"&gt;Louis Tiao&lt;/a&gt; </copyright><lastBuildDate>Sun, 22 Jul 2018 06:48:22 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Inference in Variational Autoencoders with Different Monte Carlo Sample Sizes (Addendum)</title><link>http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes-addendum/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;div class="admonition admonition-draft"&gt;
&lt;p class="first admonition-title"&gt;Draft&lt;/p&gt;
&lt;p class="last"&gt;Please do not share or link.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is a short addendum to a previous post that demonstrates how to perform
&lt;a class="reference external" href="http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/"&gt;Inference in Variational Autoencoders with Different Monte Carlo Sample Sizes&lt;/a&gt;
using the basic modular framework we developed in an &lt;a class="reference external" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"&gt;earlier post&lt;/a&gt;.&lt;/p&gt;
&lt;div class="figure align-center"&gt;
&lt;object data="http://louistiao.me/images/vae/nelbo_batch_vs_mc_sample_sizes.svg" style="height: 200px;" type="image/svg+xml"&gt;
../../images/vae/nelbo_batch_vs_mc_sample_sizes.svg&lt;/object&gt;
&lt;p class="caption"&gt;The negative evidence lower bound (ELBO) plotted after each training epoch
for various combinations of batch and Monte Carlo sample sizes.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes-addendum/"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian</category><category>deep learning</category><category>keras</category><category>mathjax</category><category>python</category><category>representation learning</category><category>tensorflow</category><category>unsupervised learning</category><category>variational autoencoder</category><category>variational inference</category><guid>http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes-addendum/</guid><pubDate>Sun, 26 Nov 2017 13:47:03 GMT</pubDate></item><item><title>Inference in Variational Autoencoders with Different Monte Carlo Sample Sizes</title><link>http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/</link><dc:creator>Louis Tiao</dc:creator><description>&lt;div&gt;&lt;p&gt;In a &lt;a class="reference external" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"&gt;previous post&lt;/a&gt;,
I demonstrated how to leverage Keras' modular design to implement variational
autoencoders in a way that makes it easy to tweak hyperparameters, adapt to it
to other related models, and extend it to the more sophisticated methods
proposed in the current research.&lt;/p&gt;
&lt;p&gt;Recall that we optimize the generally intractable evidence lower bound (ELBO)
using reparameterization gradients, which approximates the expectation of
gradients with Monte Carlo (MC) samples. In their original paper, Kingma and
Welling (2014) &lt;a class="footnote-reference" href="http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/#kingma2014" id="id1"&gt;[1]&lt;/a&gt; remark that an MC sample size of 1 is adequate for
a sufficiently large batch size (~100). Obviously, this is highly dependent on
the problem (more specifically the likelihood). In general, it is important to
experiment with different MC sample sizes and observe the various effects it
has on training stability. In this short post, we demonstrate how to tweak the
MC sample size under our basic framework.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian</category><category>deep learning</category><category>keras</category><category>mathjax</category><category>python</category><category>representation learning</category><category>tensorflow</category><category>unsupervised learning</category><category>variational autoencoder</category><category>variational inference</category><guid>http://louistiao.me/posts/inference-in-variational-autoencoders-with-different-monte-carlo-sample-sizes/</guid><pubDate>Mon, 20 Nov 2017 12:51:24 GMT</pubDate></item></channel></rss>