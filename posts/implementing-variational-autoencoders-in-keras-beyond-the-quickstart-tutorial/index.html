<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial | Louis Tiao</title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/override_nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/">
<link rel="icon" href="../../favicon_16x16.ico" sizes="16x16">
<link rel="icon" href="../../favicon_32x32.ico" sizes="32x32">
<link rel="icon" href="../../favicon_256x256.ico" sizes="256x256">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><!--

    /\\\\\\\\\\\\\\\  /\\\\\\\\\\\     /\\\\\\\\\          /\\\\\              
    \///////\\\/////  \/////\\\///    /\\\\\\\\\\\\\      /\\\///\\\           
           \/\\\           \/\\\      /\\\/////////\\\   /\\\/  \///\\\        
            \/\\\           \/\\\     \/\\\       \/\\\  /\\\      \//\\\      
             \/\\\           \/\\\     \/\\\\\\\\\\\\\\\ \/\\\       \/\\\     
              \/\\\           \/\\\     \/\\\/////////\\\ \//\\\      /\\\     
               \/\\\           \/\\\     \/\\\       \/\\\  \///\\\  /\\\      
                \/\\\        /\\\\\\\\\\\ \/\\\       \/\\\    \///\\\\\/      
                 \///        \///////////  \///        \///       \/////       

--><meta name="author" content="Louis Tiao">
<link rel="prev" href="../../notes/working-with-samples-of-distributions-over-convolutional-kernels/" title="Working with Samples of Distributions over Convolutional Kernels" type="text/html">
<link rel="next" href="../../notes/working-with-pandas-multiindex-dataframes-reading-and-writing-to-csv-and-hdf5/" title="Working with Pandas MultiIndex Dataframes: Reading and Writing to CSV and HDF5" type="text/html">
<meta property="og:site_name" content="Louis Tiao">
<meta property="og:title" content="Implementing Variational Autoencoders in Keras: Beyond the Quickstart ">
<meta property="og:url" content="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/">
<meta property="og:description" content="Draft
Please do not share or link.

Keras is awesome. It is a very well-designed library that clearly abides by
its guiding principles of modularity and extensibility, and allows us to
easily assemble">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-10-23T01:19:59+11:00">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="keras">
<meta property="article:tag" content="mathjax">
<meta property="article:tag" content="python">
<meta property="article:tag" content="representation learning">
<meta property="article:tag" content="tensorflow">
<meta property="article:tag" content="unsupervised learning">
<meta property="article:tag" content="variational autoencoder">
<meta property="article:tag" content="variational inference">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

  <div class="container">
    <div class="header clearfix">
      <nav><ul class="nav nav-pills pull-right">
<li>
<a href="../../">About</a>
                </li>
<li>
<a href="../../projects/">Projects</a>
                </li>
<li>
<a href="../">Posts</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

          
        </li>
</ul></nav><a href="http://louistiao.me/">

        <h3 class="text-muted">
          <span id="blog-title">Louis Tiao</span>
        </h3>
      </a>
    </div>

<!-- TODO Figure out what to do with this stuff -->
<!--     <div class="row">

      <ul class="nav nav-pills pull-right">
    <li>
    <a href="/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/index.rst" id="sourcelink">Source</a>
    </li>
          
      </ul>
    </div> -->

    <div id="content" role="main">
      <div class="body-content">
        <!--Body content-->
        <div class="row">
          
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Louis Tiao
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2017-10-23T01:19:59+11:00" itemprop="datePublished" title="2017-10-23 01:19">2017-10-23 01:19</time></a></p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/content/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial.html">Comments</a>


                    </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="admonition admonition-draft">
<p class="first admonition-title">Draft</p>
<p class="last">Please do not share or link.</p>
</div>
<p><a class="reference external" href="https://keras.io/">Keras</a> is awesome. It is a very well-designed library that clearly abides by
its <a class="reference external" href="https://keras.io/#guiding-principles">guiding principles</a> of modularity and extensibility, and allows us to
easily assemble powerful, complex models from primitive building blocks.
This has been demonstrated in numerous blog posts and tutorials, in particular,
the excellent tutorial on <a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a>.
As the name suggests, that tutorial provides examples of how to implement
various kinds of autoencoders in Keras, including the variational autoencoder
(VAE) <a class="footnote-reference" href="#kingma2014" id="id1">[1]</a>.</p>
<div class="figure align-center">
<img alt="../../images/vae/result_combined.png" src="../../images/vae/result_combined.png"><p class="caption">Visualization of 2D manifold of MNIST digits (left)
and the representation of digits in latent space colored according to their
digit labels (right).</p>
</div>
<p>Like all autoencoders, the variational autoencoder is primarily used for
unsupervised learning of hidden representations.
However, they are fundamentally different to your usual neural network-based
autoencoder in that they approach the problem from a probabilistic perspective.
They specify a joint distribution over the observed and latent variables, and
approximate the intractable posterior conditional density over latent
variables with variational inference, using an <em>inference network</em>
<a class="footnote-reference" href="#inference1" id="id2">[2]</a> <a class="footnote-reference" href="#inference2" id="id3">[3]</a> (or more classically, a <em>recognition model</em>
<a class="footnote-reference" href="#dayan1995" id="id4">[4]</a>) to amortize the cost of inference.</p>
<!-- TEASER_END -->
<p>While the examples in the aforementioned tutorial do well to showcase the
versatility of Keras on a wide range of autoencoder model architectures,
<a class="reference external" href="https://github.com/fchollet/keras/blob/2.0.8/examples/variational_autoencoder.py">its implementation of the variational autoencoder</a> doesn't properly take
advantage of Keras' modular design, making it difficult to generalize and
extend in important ways. As we will see, it relies on implementing custom
layers and constructs that are restricted to a specific instance of
variational autoencoders. This is a shame because when combined, Keras'
building blocks are powerful enough to encapsulate most variants of the
variational autoencoder and more generally, recognition-generative model
combinations for which the generative model belongs to a large family of
<em>deep latent Gaussian models</em> (DLGMs) <a class="footnote-reference" href="#rezende2014" id="id5">[5]</a>.</p>
<p>The goal of this post is to propose a clean and elegant alternative
implementation that takes better advantage of Keras' modular design.
It is not intended as tutorial on variational autoencoders <a class="footnote-reference" href="#id17" id="id6">[*]</a>.
Rather, we study variational autoencoders as a specific case of variational
inference in deep latent Gaussian models with inference networks, and
demonstrate how we can use Keras to implement them in a modular fashion such
that they can be easily adapted to approximate inference in various common
problems with different (non-Gaussian) likelihoods, such as classification with
Bayesian logistic / softmax regression.</p>
<p>This first post will lay the groundwork for a series of future posts that
explore ways to extend this basic modular framework to implement the more
powerful methods proposed in the latest research, such as the normalizing flows
for building richer posterior approximations <a class="footnote-reference" href="#rezende2015" id="id7">[6]</a>, importance weighted
autoencoders <a class="footnote-reference" href="#burda2015" id="id8">[7]</a>, the Gumbel-softmax trick for inference in discrete
latent variables <a class="footnote-reference" href="#jang2016" id="id9">[8]</a>, and even the most recent GAN-based density-ratio
estimation techniques for likelihood-free inference <a class="footnote-reference" href="#mescheder2017" id="id10">[9]</a> <a class="footnote-reference" href="#tran2017" id="id11">[10]</a>.</p>
<div class="section" id="model-specification">
<h2>Model specification</h2>
<p>First, it is important to understand that the variational autoencoder
<a class="reference external" href="http://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models">is not a way to train generative models</a>.
Rather, the generative model is a component of the variational autoencoder and
is, in general, a deep latent Gaussian model.
In particular, let <span class="math">\(\mathbf{x}\)</span> be a local observed variable and
<span class="math">\(\mathbf{z}\)</span> its corresponding local latent variable, with joint
distribution</p>
<div class="math">
\begin{equation*}
p_{\theta}(\mathbf{x}, \mathbf{z})
= p_{\theta}(\mathbf{x} | \mathbf{z}) p(\mathbf{z}).
\end{equation*}
</div>
<p>In Bayesian modelling, we assume the distribution of observed variables to be
governed by the latent variables. Latent variables are drawn from a prior
density <span class="math">\(p(\mathbf{z})\)</span> and related to the observations though the
likelihood <span class="math">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span>.
Deep latent Gaussian models (DLGMs) are a general class of models where the
observed variable is governed by a <em>hierarchy</em> of latent variables, and the
latent variables at each level of the hierarchy are Gaussian <em>a priori</em>
<a class="footnote-reference" href="#rezende2014" id="id12">[5]</a>.</p>
<p>In a typical instance of the variational autoencoder, we have only a single
layer of latent variables with a Normal prior distribution,</p>
<div class="math">
\begin{equation*}
p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}).
\end{equation*}
</div>
<p>Now, each local latent variable is related to its corresponding observation
through the likelihood <span class="math">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span>, which can
be viewed as a <em>probabilistic</em> decoder. Given a hidden lower-dimensional
representation (or "code") <span class="math">\(\mathbf{z}\)</span>, it "decodes" it into a
<em>distribution</em> over the observation <span class="math">\(\mathbf{x}\)</span>.</p>
<div class="section" id="decoder">
<h3>Decoder</h3>
<p>In this example, we define <span class="math">\(p_{\theta}(\mathbf{x} | \mathbf{z})\)</span> to
be a multivariate Bernoulli whose probabilities are computed from
<span class="math">\(\mathbf{z}\)</span> using a fully-connected neural network with a single hidden
layer,</p>
<div class="math">
\begin{align*}
p_{\theta}(\mathbf{x} | \mathbf{z})
  &amp; = \mathrm{Bern}( \sigma( \mathbf{W}_2 \mathbf{h} + \mathbf{b}_2 ) ), \\
\mathbf{h} &amp; = h(\mathbf{W}_1 \mathbf{z} + \mathbf{b}_1),
\end{align*}
</div>
<p>where <span class="math">\(\sigma\)</span> is the logistic sigmoid function, <span class="math">\(h\)</span> is some
non-linearity, and the model parameters
<span class="math">\(\theta = \{ \mathbf{W}_1, \mathbf{W}_2, \mathbf{b}_1, \mathbf{b}_1 \}\)</span>
consist of the weights and biases of this neural network.</p>
<p>It is straightforward to implement this in Keras with the
<a class="reference external" href="https://keras.io/models/sequential/">Sequential model API</a>:</p>
<pre class="code python"><a name="rest_code_1fec4e985ef44e2b9bfd1a446ff1cbe7-1"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_1fec4e985ef44e2b9bfd1a446ff1cbe7-2"></a>  <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_1fec4e985ef44e2b9bfd1a446ff1cbe7-3"></a>  <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_1fec4e985ef44e2b9bfd1a446ff1cbe7-4"></a><span class="p">])</span>
</pre>
<p>You can view a summary of the model parameters <span class="math">\(\theta\)</span> by calling
<tt class="docutils literal">decoder.summary()</tt>. Additionally, you can produce a high-level diagram of
the network architecture, and optionally the input and output shapes of each
layer using <a class="reference external" href="https://keras.io/visualization/">plot_model</a> from the
<tt class="docutils literal">keras.utils.vis_utils</tt> module. Although our architecture is about as
simple as it gets, it is included in the figure below as an example of what
the diagrams look like.</p>
<div class="figure align-center">
<object data="../../images/vae/decoder.svg" style="height: 200px;" type="image/svg+xml">
../../images/vae/decoder.svg</object>
<p class="caption">Decoder architecture.</p>
</div>
<p>Note that by fixing <span class="math">\(\mathbf{W}_1\)</span>, <span class="math">\(\mathbf{b}_1\)</span> and <span class="math">\(h\)</span>
to be the identity matrix, the zero vector, and the identity function,
respectively (or equivalently dropping the first <tt class="docutils literal">Dense</tt> layer in the snippet
above altogether), we recover <em>logistic factor analysis</em>.
With similarly minor modifications, we can recover other members from the
family of DLGMs, which include <em>non-linear factor analysis</em>,
<em>non-linear Gaussian belief networks</em>, <em>sigmoid belief networks</em>, and many
others <a class="footnote-reference" href="#rezende2014" id="id13">[5]</a>.</p>
<pre class="code python"><a name="rest_code_b52473084ee940f78980f0ec6f7c9bb3-1"></a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<a name="rest_code_b52473084ee940f78980f0ec6f7c9bb3-2"></a>    <span class="sd">""" Negative log likelihood (Bernoulli). """</span>
<a name="rest_code_b52473084ee940f78980f0ec6f7c9bb3-3"></a>
<a name="rest_code_b52473084ee940f78980f0ec6f7c9bb3-4"></a>    <span class="c1"># keras.losses.binary_crossentropy give the mean</span>
<a name="rest_code_b52473084ee940f78980f0ec6f7c9bb3-5"></a>    <span class="c1"># over the last axis. we require the sum</span>
<a name="rest_code_b52473084ee940f78980f0ec6f7c9bb3-6"></a>    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p>If you are using the TensorFlow backend, you can directly use the
(negative) log probability of <tt class="docutils literal">Bernoulli</tt> from TensorFlow Distributions as
a Keras loss, as I demonstrate in my post on
<a class="reference external" href="../using-negative-log-likelihoods-of-tensorflow-distributions-as-keras-losses/">Using negative log-likelihoods of TensorFlow Distributions as Keras losses</a>.</p>
<p>That is, the following is equivalent to the above definition which instead
uses the <tt class="docutils literal">K.binary_crossentropy</tt> function:</p>
<div class="last"><pre class="code python"><a name="rest_code_7f04233a7a754dacbc301bee3f525be9-1"></a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<a name="rest_code_7f04233a7a754dacbc301bee3f525be9-2"></a>    <span class="sd">""" Negative log likelihood (Bernoulli). """</span>
<a name="rest_code_7f04233a7a754dacbc301bee3f525be9-3"></a>
<a name="rest_code_7f04233a7a754dacbc301bee3f525be9-4"></a>    <span class="n">lh</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">tf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
<a name="rest_code_7f04233a7a754dacbc301bee3f525be9-5"></a>
<a name="rest_code_7f04233a7a754dacbc301bee3f525be9-6"></a>    <span class="k">return</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lh</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">y_true</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="inference">
<h2>Inference</h2>
<p>Having specified the generative process, we would now like to perform inference
on the latent variables and model parameters <span class="math">\(\mathbf{z}\)</span> and
<span class="math">\(\theta\)</span>, respectively.
In particular, our goal is to compute the posterior
<span class="math">\(p_{\theta}(\mathbf{z} | \mathbf{x})\)</span>, the conditional density of the
latent variable <span class="math">\(\mathbf{z}\)</span> given observed variable <span class="math">\(\mathbf{x}\)</span>.
Additionally, we wish to optimize the model parameters <span class="math">\(\theta\)</span> with
respect to the marginal likelihood <span class="math">\(p_{\theta}(\mathbf{x})\)</span>.
Both depend on the marginal likelihood, whose calculation requires marginalizing
out the latent variables <span class="math">\(\mathbf{z}\)</span>. In general, this is computational
intractable, requiring exponential time to compute. Or, it is analytically
intractable and cannot be evaluated in closed-form, as it is in our case
where the Gaussian prior is non-conjugate to the Bernoulli likelihood.</p>
<p>To circumvent this intractability we turn to variational inference, which
formulates inference as an optimization problem. It seeks an approximate
posterior <span class="math">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span> closest in Kullback-Leibler
(KL) divergence to the true posterior. More precisely, the approximate posterior
is parameterized by <em>variational parameters</em> <span class="math">\(\phi\)</span>, and we seek a setting
of these parameters that minimizes the aforementioned KL divergence,</p>
<div class="math">
\begin{equation*}
\phi^* = \mathrm{argmin}_{\phi}
\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z} | \mathbf{x}) ]
\end{equation*}
</div>
<p>With the luck we've had so far, it shouldn't come as a surprise anymore that
<em>this too</em> is intractable. It also depends on the log marginal likelihood,
whose intractability is the reason we appealed to approximate inference in the
first place. Instead, we <em>maximize</em> an alternative objective function, the
<em>evidence lower bound</em> (ELBO), which is expressed as</p>
<div class="math">
\begin{align*}
\mathrm{ELBO}(q)
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z}) +
  \log p(\mathbf{z}) -
  \log q_{\phi}(\mathbf{z} | \mathbf{x})
] \\
&amp;=
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [
  \log p_{\theta}(\mathbf{x} | \mathbf{z})
] - \mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) ].
\end{align*}
</div>
<p>Importantly, the ELBO is a lower bound to the log marginal likelihood.
Therefore, maximizing it with respect to the model parameters <span class="math">\(\theta\)</span>
approximately maximizes the log marginal likelihood.
Additionally, maximizing it with respect variational parameter <span class="math">\(\phi\)</span> can
be shown to minimize
<span class="math">\(\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z} | \mathbf{x}) ]\)</span>. Also, it turns out that the KL divergence determines the
tightness of the lower bound, where we have equality iff the KL divergence is
zero, which happens iff
<span class="math">\(q_{\phi}(\mathbf{z} | \mathbf{x}) = p_{\theta}(\mathbf{z} | \mathbf{x})\)</span>.
Hence, simultaneously maximizing it with respect to <span class="math">\(\theta\)</span> and
<span class="math">\(\phi\)</span> gets us two birds with one stone.</p>
<p>Next we discuss the form of the approximate posterior
<span class="math">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>, which can be viewed as a
<em>probabilistic</em> encoder. Its role is opposite to that of the decoder.
Given an observation <span class="math">\(\mathbf{x}\)</span>, it "encodes" it into a <em>distribution</em>
over its hidden lower-dimensional representations.</p>
<div class="section" id="encoder">
<h3>Encoder</h3>
<p>For each local observed variable <span class="math">\(\mathbf{x}_n\)</span>, we wish to approximate
the true posterior distribution <span class="math">\(p(\mathbf{z}_n|\mathbf{x}_n)\)</span> over its
corresponding local latent variables <span class="math">\(\mathbf{z}_n\)</span>. A common approach it
to approximate it using a variational distribution
<span class="math">\(q_{\phi_n}(\mathbf{z}_n | \mathbf{x}_n)\)</span> that is a diagonal Gaussian,
where the <em>local</em> variational parameters
<span class="math">\(\phi_n = \{ \mathbf{\mu}_n, \mathbf{\sigma}_n \}\)</span> are the means and
variances of this approximating distribution,</p>
<div class="math">
\begin{equation*}
q_{\phi_n}(\mathbf{z}_n | \mathbf{x}_n) =
\mathcal{N}(
  \mathbf{z}_n |
  \mathbf{\mu}_n,
  \mathrm{diag}(\mathbf{\sigma}_n^2)
).
\end{equation*}
</div>
<p>This approach has a number of shortcomings. First, the number of local
variational parameters we are required to optimize grows with the size of the
dataset. Second, a new set of local variational parameters need to be optimized
for new unseen test points. This is not to mention the strong factorization
assumption we make by specifying diagonal Gaussian distributions as the family
of approximations.</p>
<div class="section" id="inference-network">
<h4>Inference network</h4>
<p>We <em>amortize</em> the cost of inference by introducing an <em>inference network</em> which
approximates the local variational parameters <span class="math">\(\phi_n\)</span> for a given local
observed variable <span class="math">\(\textbf{x}_n\)</span>.
For our approximating distribution in particular, given <span class="math">\(\textbf{x}_n\)</span> the
inference network yields two outputs <span class="math">\(\mu_{\phi}(\textbf{x}_n)\)</span> and
<span class="math">\(\sigma_{\phi}(\textbf{x}_n)\)</span>, which we use to approximate its local
variational parameters <span class="math">\(\mathbf{\mu}_n\)</span> and <span class="math">\(\mathbf{\sigma}_n\)</span>,
respectively.
Our approximate posterior distribution now becomes</p>
<div class="math">
\begin{equation*}
q_{\phi}(\mathbf{z} | \mathbf{x})
=
\mathcal{N}(
  \mathbf{z} |
  \mathbf{\mu}_{\phi}(\mathbf{x}),
  \mathrm{diag}(\mathbf{\sigma}_{\phi}^2(\mathbf{x}))
).
\end{equation*}
</div>
<p>Instead of learning local variational parameters <span class="math">\(\phi_n\)</span> for each
data-point, we now learn a fixed number of <em>global</em> variational parameters
<span class="math">\(\phi\)</span> which constitute the parameters of the inference network.
Moreover, this approximation allows statistical strength to be shared across
observed data-points and also generalize to unseen test points.</p>
<p>We specify the location and scale of this distribution as the output of an
inference network. For this post, we keep the architecture of the network
simple, with only a single hidden layer and two fully-connected output layers.
Again, this is simple to define in Keras:</p>
<pre class="code python"><a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-1"></a><span class="c1"># input layer</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-2"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-3"></a>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-4"></a><span class="c1"># hidden layer</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-5"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-6"></a>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-7"></a><span class="c1"># output layer for mu</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-8"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-9"></a>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-10"></a><span class="c1"># output layer for sigma</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-11"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_cbeb78e1f90541dc9428a536920d9b4e-12"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
</pre>
<p>Since this network has multiple outputs, we couldn't use the Sequential model
API as we did for the decoder. Instead, we must resort to the more powerful
<a class="reference external" href="https://keras.io/getting-started/functional-api-guide/">Functional API</a>,
which allows you to implement complex models with shared layers, multiple
inputs, multiple outputs, and so on.</p>
<p>To provide some context, we remark that inference networks are more classically
known as <em>recognition models</em>. When combined end-to-end, the recognition and
generative model can be seen as having an autoencoder structure.
Indeed, this structure contains the variational autoencoder as a special case,
and more classically, the <em>Helmholtz machine</em> <a class="footnote-reference" href="#dayan1995" id="id14">[4]</a>.
More generally, we can use this structure to perform approximate Bayesian
inference in models that lie beyond even the large class of deep latent Gaussian
models. Hence, referring to it as a probabilistic encoder, though an accurate
interpretation, is a limited one.</p>
<!-- TODO -->
<!-- **Figure here** -->
<!-- DONE cannot use Sequential model API -->
<!-- Lambda layer -->
<!-- Reference to Helmholtz machines, which has a recognition model and inference -->
<!-- is done using the wake-sleep algorithm. -->
<!-- Note that it is not dependent on the observed data x_i -->
<!-- and does not appear in the expression q_i(z_i). It is only related to x_i -->
<!-- through the ELBO. -->
</div>
<div class="section" id="kl-divergence">
<h4>KL Divergence</h4>
<p>latent space regularization</p>
<div class="math">
\begin{equation*}
\mathrm{KL} [q_{\phi}(\mathbf{z} | \mathbf{x}) \| p(\mathbf{z}) ]
= - \frac{1}{2} \sum_{k=1}^K \{ 1 + \log \sigma_k^2 - \mu_k^2 - \sigma_k^2 \}
\end{equation*}
</div>
<pre class="code python"><a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-1"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-2"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-3"></a>    <span class="sd">""" Identity transform layer that adds KL divergence</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-4"></a><span class="sd">    to the final model loss.</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-5"></a><span class="sd">    """</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-6"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-7"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-8"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-9"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-10"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-11"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-12"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-13"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-14"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-15"></a>        <span class="n">kl_batch</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-16"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-17"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-18"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-19"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_batch</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-20"></a>
<a name="rest_code_bf969aabc1ad4e25a163b901425c34b9-21"></a>        <span class="k">return</span> <span class="n">inputs</span>
</pre>
<pre class="code python"><a name="rest_code_c4ccec0be3984cd6aee48938c015530d-1"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
</pre>
<p>by itself, it will learn to ignore the input and map all outputs to 0.
It is only when we tack on the decoder that the reconstruction likelihood
is introduced. Only then will we reconcile the likelihood / observed data with
our prior to form the posterior over latent codes.</p>
<p>At this stage we could specify
<tt class="docutils literal">prob_encoder = Model(inputs=x, <span class="pre">outputs=[z_mu,</span> z_sigma])</tt>
and compile it with something like
<tt class="docutils literal"><span class="pre">prob_encoder.compile(optimizer='rmsprop`,</span> loss=None)</tt>.
When we fit it, it would trivially map all inputs to 0 and 1, thus learning the
prior distribution.</p>
<p>inputs mu and log_var are of shape (batch_size, latent_dim)
the loss we add should be scalar. this is unlike loss
function specified in model compile which should returns
loss vector of shape (batch_size,) since it requires
loss for each datapoint in the batch for sample
weighting.</p>
</div>
<div class="section" id="reparameterization-using-merge-layers">
<h4>Reparameterization using Merge Layers</h4>
<p>To perform gradient-based optimization of ELBO with respect to model parameters
<span class="math">\(theta\)</span> and variational parameters <span class="math">\(\phi\)</span>, we are required to
compute its gradients with respect to these parameters. This is easy to do for
parameters <span class="math">\(theta\)</span>, but is generally intractable for parameters
<span class="math">\(\phi\)</span>. Currently, the dominant approach for circumventing this is by
Monte Carlo (MC) estimation of the gradients. There exist a number of estimators
based on different variance reduction techniques. However, the
<em>reparameterization gradients</em> can be shown to have the lowest variance among
competing estimators for continuous latent variables,</p>
<p>The ELBO can be written as an expectation of a multivariate function
<span class="math">\(f(\mathbf{x}, \mathbf{z}) = \log p_{\theta}(\mathbf{x} , \mathbf{z}) - \log q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>
over distribution <span class="math">\(q_{\phi}(\mathbf{z} | \mathbf{x})\)</span>.</p>
<div class="math">
\begin{align*}
\nabla_{\phi}
\mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})} [ f(\mathbf{x}, \mathbf{z}) ]
&amp;= \nabla_{\phi} \mathbb{E}_{p(\mathbf{\epsilon})} [
   f(\mathbf{x},
     g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
&amp;= \mathbb{E}_{p(\mathbf{\epsilon})} [
 \nabla_{\phi}
 f(\mathbf{x},
   g_{\phi}(\mathbf{x}, \mathbf{\epsilon}))
] \\
\end{align*}
</div>
<p>Specifying  gives us the gradient of the ELBO above.</p>
<div class="math">
\begin{equation*}
z = g_{\phi}(\mathbf{x}, \mathbf{\epsilon}), \quad
  \mathbf{\epsilon} \sim p(\mathbf{\epsilon})
\end{equation*}
</div>
<div class="math">
\begin{equation*}
g_{\phi}(\mathbf{x}, \mathbf{\epsilon}) =
  \mathbf{\mu}_{\phi}(\mathbf{x}) +
  \mathbf{\sigma}_{\phi}(\mathbf{x}) \odot
  \mathbf{\epsilon}, \quad
  \mathbf{\epsilon} \sim
  \mathcal{N}(\mathbf{0}, \mathbf{I})
\end{equation*}
</div>
<p>Assume <tt class="docutils literal">z_mu</tt> and <tt class="docutils literal">z_sigma</tt> are the outputs of some layers. Then, using
<a class="reference external" href="https://keras.io/layers/merge/">Merge Layers</a>, <tt class="docutils literal">Add</tt> and <tt class="docutils literal">Multiply</tt>:</p>
<pre class="code python"><a name="rest_code_d537aef5993d41129c7c4ee2a6a0d652-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
<a name="rest_code_d537aef5993d41129c7c4ee2a6a0d652-2"></a>
<a name="rest_code_d537aef5993d41129c7c4ee2a6a0d652-3"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_d537aef5993d41129c7c4ee2a6a0d652-4"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/reparameterization.svg" style="height: 300px;" type="image/svg+xml">
../../images/vae/reparameterization.svg</object>
<p class="caption">Reparameterization with simple location-scale transformation using Keras
merge layers.</p>
</div>
<p>Lambda layer, which simultaneously draws samples from a hard-coded base
distribution and performs reparameterization. This implementation achieves a
more appropriate level of modularity and abstraction. It's makes it clear that
each of these atomic building blocks are themselves deterministic
transformations which together make up a deterministic transformation.
The source of stochasticity comes from the input, which we are able to tweak at
test time. Gumbel-softmax trick.</p>
<pre class="code python"><a name="rest_code_78fd43b2c5514216b50050d86f211014-1"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
</pre>
<p>For the sake of illustration, we've fixed <tt class="docutils literal">sigma</tt> and <tt class="docutils literal">mu</tt> as <tt class="docutils literal">Input</tt>
layers. That's why it says <tt class="docutils literal">InputLayer</tt> next to it. In reality, it will be
the output layer of a network. We specify <span class="math">\(\mathbf{\mu}_{\phi}(\mathbf{x})\)</span>
and <span class="math">\(\mathbf{\sigma}_{\phi}(\mathbf{x})\)</span> now.</p>
<div class="figure align-center">
<object data="../../images/vae/encoder.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder.svg</object>
<p class="caption">Encoder architecture.</p>
</div>
<div class="figure align-center">
<object data="../../images/vae/encoder_full.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/encoder_full.svg</object>
<p class="caption">Full encoder architecture, including auxiliary KL divergence layer.</p>
</div>
</div>
</div>
<div class="section" id="putting-it-all-together">
<h3>Putting it all together</h3>
<pre class="code python"><a name="rest_code_431b234ff6a844579925a628d58a09e4-1"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-2"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-3"></a>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-4"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-5"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-6"></a>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-7"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-8"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-9"></a>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-10"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-11"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-12"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-13"></a>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-14"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-15"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-16"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-17"></a><span class="p">])</span>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-18"></a>
<a name="rest_code_431b234ff6a844579925a628d58a09e4-19"></a><span class="n">x_pred</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre>
<pre class="code python"><a name="rest_code_bbbdb44207ce437fa2127bf6bde773f9-1"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x_pred</span><span class="p">)</span>
<a name="rest_code_bbbdb44207ce437fa2127bf6bde773f9-2"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/vae_full_shapes.svg" style="height: 500px;" type="image/svg+xml">
../../images/vae/vae_full_shapes.svg</object>
<p class="caption">Variational autoencoder architecture.</p>
</div>
<p>The point of this tutorial is to illustrate the general framework for performing
amortized variational inference using Keras, treating the inference network
(approximate posterior) and the generative network (likelihood) as black-boxes.
What we've used for the encoder and decoder each with a single hidden
full-connected layer is perhaps the minimal viable architecture.
In the examples directory, Keras provides a more sophisticated variational
autoencoder with deconvolutional layers. The architecture definitions can be
trivially copy-pasted here without need to modify anything else.</p>
</div>
</div>
<div class="section" id="model-fitting">
<h2>Model fitting</h2>
<p>We load the training data as usual. Now the <tt class="docutils literal">vae</tt> is explicitly specified with
random noise source as an auxiliary input. This allows to easily control the
base distribution <span class="math">\(p(\mathbf{\epsilon})\)</span> and also how we draw Monte Carlo
samples of <span class="math">\(\mathbf{z}\)</span> for each datapoint <span class="math">\(\mathbf{x}\)</span>. Usually
we just stick with a simple isotropic Gaussian distribution and draw a different
MC sample for each datapoint.</p>
<pre class="code python"><a name="rest_code_c3347a9f970e4c3997e359c50d19c784-1"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_c3347a9f970e4c3997e359c50d19c784-2"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_c3347a9f970e4c3997e359c50d19c784-3"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
</pre>
<p>Model fitting feels less intuitive. The <tt class="docutils literal">vae</tt> is compiled with <tt class="docutils literal">loss=None</tt>
explicitly specified which raises a warning. When fit is called, the targets
argument is left unspecified, and the reconstruction loss is optimized through
the <cite>CustomLayer</cite>. This mapping from mathematical problem formulation to code
implementation appears more natural and straightforward. It's easy to understand
at a glance from our call to the <tt class="docutils literal">fit</tt> method that we're training a
probabilistic auto-encoder.</p>
<pre class="code python"><a name="rest_code_1c5f9b7475d242038a3e3a2842f54fbf-1"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_1c5f9b7475d242038a3e3a2842f54fbf-2"></a>        <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_1c5f9b7475d242038a3e3a2842f54fbf-3"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_1c5f9b7475d242038a3e3a2842f54fbf-4"></a>        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_1c5f9b7475d242038a3e3a2842f54fbf-5"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_1c5f9b7475d242038a3e3a2842f54fbf-6"></a>        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
</pre>
<p>Personally, I prefer this view since the all sources of stochasticity emanate
from the inputs to the model.</p>
<div class="section" id="loss-nelbo-convergence">
<h3>Loss (NELBO) Convergence</h3>
<pre class="code python"><a name="rest_code_55d8373ed33744c6962ffed9b39168d7-1"></a><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-2"></a>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-3"></a><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">hist</span><span class="o">.</span><span class="n">history</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-4"></a>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-5"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'NELBO'</span><span class="p">)</span>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-6"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'# epochs'</span><span class="p">)</span>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-7"></a>
<a name="rest_code_55d8373ed33744c6962ffed9b39168d7-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<object data="../../images/vae/nelbo.svg" style="width: 500px;" type="image/svg+xml">
../../images/vae/nelbo.svg</object>
</div>
</div>
</div>
<div class="section" id="model-evaluation">
<h2>Model evaluation</h2>
<pre class="code python"><a name="rest_code_70c038acff18480baf3e108543cb28fc-1"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-2"></a>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-3"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-4"></a><span class="n">z_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-5"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-6"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-7"></a>            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-8"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_70c038acff18480baf3e108543cb28fc-9"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<img alt="../../images/vae/result_latent_space.png" src="../../images/vae/result_latent_space.png" style="height: 500px;">
</div>
<pre class="code python"><a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-1"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-2"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-3"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-4"></a>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-5"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-6"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-7"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-8"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-9"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-10"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-11"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-12"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-13"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-14"></a>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-15"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-16"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_fc8e40bb39b143f8a81684b2d2833025-17"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
<div class="figure align-center">
<img alt="../../images/vae/result_manifold.png" src="../../images/vae/result_manifold.png" style="height: 600px;">
</div>
</div>
<div class="section" id="recap">
<h2>Recap</h2>
<ul class="simple">
<li>Demonstration of Sequential and functional Model API</li>
<li>Custom auxiliary layers that augments the model loss</li>
<li>Fixing input to source of stochasticity</li>
<li>Reparameterization using Merge layers</li>
</ul>
</div>
<div class="section" id="what-s-next">
<h2>What's next</h2>
<p>Normalizing flows</p>
<p>We illustrate how to employ the simple Gumbel-Softmax reparameterization to
build a Categorical VAE with discrete latent variables.</p>
<p>We can easily extend <tt class="docutils literal">KLDivergenceLayer</tt> to use an auxiliary density ratio
estimator function, instead of evaluating the KL divergence in the closed-form
expression above.
This relaxes the requirement on approximate posterior
<span class="math">\(q(\mathbf{z}|\mathbf{x})\)</span> (and incidentally, also prior
<span class="math">\(p(\mathbf{z})\)</span>) to yield tractable densities, at the cost of maximizing
a cruder estimate of the ELBO.
This is known as Adversarial Variational Bayes <a class="footnote-reference" href="#mescheder2017" id="id15">[9]</a>, and is an
important line of recent research that extends the applicability of variational
inference to arbitrarily expressive implicit probabilistic models <a class="footnote-reference" href="#tran2017" id="id16">[10]</a>.</p>
</div>
<div class="section" id="footnotes">
<h2>Footnotes</h2>
<table class="docutils footnote" frame="void" id="id17" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id6">[*]</a></td>
<td>
<p class="first">For a complete treatment of variational autoencoders, and variational
inference in general, I highly recommend:</p>
<ul class="last simple">
<li>Jaan Altosaar's blog post, <a class="reference external" href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">What is a variational autoencoder?</a>.</li>
<li>Diederik P. Kingma's PhD Thesis,
<a class="reference external" href="https://www.dropbox.com/s/v6ua3d9yt44vgb3/cover_and_thesis.pdf?dl=1">Variational Inference and Deep Learning: A New Synthesis</a>.</li>
</ul>
</td>
</tr></tbody>
</table>
</div>
<div class="section" id="references">
<h2>References</h2>
<table class="docutils footnote" frame="void" id="kingma2014" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>D. P. Kingma and M. Welling,
"Auto-Encoding Variational Bayes,"
in Proceedings of the 2nd International Conference on Learning
Representations (ICLR), 2014.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="inference1" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id2">[2]</a></td>
<td><a class="reference external" href="http://edwardlib.org/tutorials/inference-networks">Edward tutorial on Inference Networks</a></td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="inference2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id3">[3]</a></td>
<td>Section "Recognition models and amortised inference" in
<a class="reference external" href="http://blog.shakirm.com/2015/01/variational-inference-tricks-of-the-trade/">Shakir's blog post</a>.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="dayan1995" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[4]</td>
<td>
<em>(<a class="fn-backref" href="#id4">1</a>, <a class="fn-backref" href="#id14">2</a>)</em> Dayan, P., Hinton, G. E., Neal, R. M., &amp; Zemel, R. S. (1995).
The Helmholtz machine. Neural Computation, 7(5), 889–904.
<a class="reference external" href="http://doi.org/10.1162/neco.1995.7.5.889">http://doi.org/10.1162/neco.1995.7.5.889</a>
</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="rezende2014" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[5]</td>
<td>
<em>(<a class="fn-backref" href="#id5">1</a>, <a class="fn-backref" href="#id12">2</a>, <a class="fn-backref" href="#id13">3</a>)</em> Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014).
"Stochastic backpropagation and approximate inference in deep generative models,"
in Proceedings of The 31st International Conference on Machine Learning, 2014,
(Vol. 32, pp. 1278–1286). Bejing, China: PMLR. <a class="reference external" href="http://doi.org/10.1051/0004-6361/201527329">http://doi.org/10.1051/0004-6361/201527329</a>
</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="rezende2015" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id7">[6]</a></td>
<td>D. Rezende and S. Mohamed,
"Variational Inference with Normalizing Flows,"
in Proceedings of the 32nd International Conference on Machine Learning, 2015,
vol. 37, pp. 1530–1538.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="burda2015" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id8">[7]</a></td>
<td>Y. Burda, R. Grosse, and R. Salakhutdinov,
"Importance Weighted Autoencoders,"
in Proceedings of the 3rd International Conference on Learning
Representations (ICLR), 2015.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="jang2016" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id9">[8]</a></td>
<td>E. Jang, S. Gu, and B. Poole,
"Categorical Reparameterization with Gumbel-Softmax," Nov. 2016.
in Proceedings of the 5th International Conference on Learning
Representations (ICLR), 2017.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="mescheder2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[9]</td>
<td>
<em>(<a class="fn-backref" href="#id10">1</a>, <a class="fn-backref" href="#id15">2</a>)</em> L. Mescheder, S. Nowozin, and A. Geiger,
"Adversarial Variational Bayes: Unifying Variational Autoencoders and
Generative Adversarial Networks,"
in Proceedings of the 34th International Conference on Machine Learning, 2017,
vol. 70, pp. 2391–2400.</td>
</tr></tbody>
</table>
<table class="docutils footnote" frame="void" id="tran2017" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label">[10]</td>
<td>
<em>(<a class="fn-backref" href="#id11">1</a>, <a class="fn-backref" href="#id16">2</a>)</em> D. Tran, R. Ranganath, and D. Blei,
"Hierarchical Implicit Models and Likelihood-Free Variational Inference,"
<em>to appear in</em> Advances in Neural Information Processing Systems 31, 2017.</td>
</tr></tbody>
</table>
</div>
<div class="section" id="appendix">
<h2>Appendix</h2>
<p>Below, you can find:</p>
<ul class="simple">
<li>The <a class="reference external" href="../../listings/vae/variational_autoencoder.ipynb.html">accompanying Jupyter Notebook</a> used to generate the diagrams and plots
in this post.</li>
<li>The above snippets combined in a single executable Python file:</li>
</ul>
<p><a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py.html">vae/variational_autoencoder_improved.py</a>  <a class="reference external" href="../../listings/vae/variational_autoencoder_improved.py">(Source)</a></p>
<pre class="code python"><a name="rest_code_aad712d783814d7bb61a75e25da3a04d-1"></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-2"></a><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-3"></a><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-4"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-5"></a><span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-6"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-7"></a><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">Add</span><span class="p">,</span> <span class="n">Multiply</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-8"></a><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Sequential</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-9"></a><span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-10"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-11"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-12"></a><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-13"></a><span class="n">original_dim</span> <span class="o">=</span> <span class="mi">784</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-14"></a><span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">2</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-15"></a><span class="n">intermediate_dim</span> <span class="o">=</span> <span class="mi">256</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-16"></a><span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-17"></a><span class="n">epsilon_std</span> <span class="o">=</span> <span class="mf">1.0</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-18"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-19"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-20"></a><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-21"></a>    <span class="sd">""" Bernoulli negative log likelihood. """</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-22"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-23"></a>    <span class="c1"># keras.losses.binary_crossentropy gives the mean</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-24"></a>    <span class="c1"># over the last axis. We require the sum.</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-25"></a>    <span class="k">return</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">binary_crossentropy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-26"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-27"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-28"></a><span class="k">class</span> <span class="nc">KLDivergenceLayer</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-29"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-30"></a>    <span class="sd">""" Identity transform layer that adds KL divergence</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-31"></a><span class="sd">    to the final model loss.</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-32"></a><span class="sd">    """</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-33"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-34"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-35"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">is_placeholder</span> <span class="o">=</span> <span class="bp">True</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-36"></a>        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivergenceLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-37"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-38"></a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-39"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-40"></a>        <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">inputs</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-41"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-42"></a>        <span class="n">kl_batch</span> <span class="o">=</span> <span class="o">-</span> <span class="o">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">K</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-43"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span> <span class="o">-</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-44"></a>                                <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_var</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-45"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-46"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">kl_batch</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-47"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-48"></a>        <span class="k">return</span> <span class="n">inputs</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-49"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-50"></a><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-51"></a><span class="n">h</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-52"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-53"></a><span class="n">z_mu</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-54"></a><span class="n">z_log_var</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">)(</span><span class="n">h</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-55"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-56"></a><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span> <span class="o">=</span> <span class="n">KLDivergenceLayer</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_log_var</span><span class="p">])</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-57"></a><span class="n">z_sigma</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="n">t</span><span class="p">))(</span><span class="n">z_log_var</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-58"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-59"></a><span class="n">eps</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="n">K</span><span class="o">.</span><span class="n">random_normal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">epsilon_std</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-60"></a>                                   <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">latent_dim</span><span class="p">)))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-61"></a><span class="n">z_eps</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">()([</span><span class="n">z_sigma</span><span class="p">,</span> <span class="n">eps</span><span class="p">])</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-62"></a><span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">z_mu</span><span class="p">,</span> <span class="n">z_eps</span><span class="p">])</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-63"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-64"></a><span class="n">decoder</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-65"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">intermediate_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-66"></a>    <span class="n">Dense</span><span class="p">(</span><span class="n">original_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-67"></a><span class="p">])</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-68"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-69"></a><span class="n">x_pred</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-70"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-71"></a><span class="n">vae</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">eps</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x_pred</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-72"></a><span class="n">vae</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">nll</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-73"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-74"></a><span class="c1"># train the VAE on MNIST digits</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-75"></a><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-76"></a><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-77"></a><span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">original_dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-78"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-79"></a><span class="n">vae</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-80"></a>        <span class="n">x_train</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-81"></a>        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-82"></a>        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-83"></a>        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-84"></a>        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-85"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-86"></a><span class="n">encoder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_mu</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-87"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-88"></a><span class="c1"># display a 2D plot of the digit classes in the latent space</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-89"></a><span class="n">z_test</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-90"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-91"></a><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z_test</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z_test</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-92"></a>            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-93"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-94"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-95"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-96"></a><span class="c1"># display a 2D manifold of the digits</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-97"></a><span class="n">n</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># figure with 15x15 digits</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-98"></a><span class="n">digit_size</span> <span class="o">=</span> <span class="mi">28</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-99"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-100"></a><span class="c1"># linearly spaced coordinates on the unit square were transformed</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-101"></a><span class="c1"># through the inverse CDF (ppf) of the Gaussian to produce values</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-102"></a><span class="c1"># of the latent variables z, since the prior of the latent space</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-103"></a><span class="c1"># is Gaussian</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-104"></a><span class="n">u_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-105"></a>                               <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="n">n</span><span class="p">)))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-106"></a><span class="n">z_grid</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">u_grid</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-107"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">z_grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-108"></a><span class="n">x_decoded</span> <span class="o">=</span> <span class="n">x_decoded</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">,</span> <span class="n">digit_size</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-109"></a>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-110"></a><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-111"></a><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="n">x_decoded</span><span class="p">))),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<a name="rest_code_aad712d783814d7bb61a75e25da3a04d-112"></a><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../tags/deep-learning/" rel="tag">deep learning</a></li>
            <li><a class="tag p-category" href="../../tags/keras/" rel="tag">keras</a></li>
            <li><a class="tag p-category" href="../../tags/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../tags/representation-learning/" rel="tag">representation learning</a></li>
            <li><a class="tag p-category" href="../../tags/tensorflow/" rel="tag">tensorflow</a></li>
            <li><a class="tag p-category" href="../../tags/unsupervised-learning/" rel="tag">unsupervised learning</a></li>
            <li><a class="tag p-category" href="../../tags/variational-autoencoder/" rel="tag">variational autoencoder</a></li>
            <li><a class="tag p-category" href="../../tags/variational-inference/" rel="tag">variational inference</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../../notes/working-with-samples-of-distributions-over-convolutional-kernels/" rel="prev" title="Working with Samples of Distributions over Convolutional Kernels">Previous post</a>
            </li>
            <li class="next">
                <a href="../../notes/working-with-pandas-multiindex-dataframes-reading-and-writing-to-csv-and-hdf5/" rel="next" title="Working with Pandas MultiIndex Dataframes: Reading and Writing to CSV and HDF5">Next post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ltiao",
            disqus_url="http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/",
        disqus_title="Implementing Variational Autoencoders in Keras: Beyond the Quickstart Tutorial",
        disqus_identifier="cache/content/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><script>var disqus_shortname="ltiao";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->
      </div>
    </div>

    <footer id="footer" class="footer">
        
Contents © 2017
<a href="mailto:louistiao@me.com">Louis Tiao</a> - Powered by
<a href="https://getnikola.com" rel="nofollow">Nikola</a>


<span class="pull-right">

  <a class="twitter-follow-button" href="https://twitter.com/louistiao" data-show-count="false" data-show-screen-name="false">
  Follow @louistiao
  </a>

  <a class="github-button" href="https://github.com/ltiao" aria-label="Follow @ltiao on GitHub" data-show-count="false">
  Follow @ltiao
  </a>

  <a href="https://ko-fi.com/A3476EX">
    <object type="image/svg+xml" style="pointer-events: none;" data="https://img.shields.io/badge/Support--yellow.svg?style=social"></object>
  </a>

</span>


            
    </footer>
</div> <!-- /container -->

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><!-- Google Analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43722566-1', 'auto');
  ga('send', 'pageview');

</script><!-- GitHub Buttons --><script async defer src="https://buttons.github.io/buttons.js"></script><!-- Twitter Widgets --><script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
</body>
</html>
