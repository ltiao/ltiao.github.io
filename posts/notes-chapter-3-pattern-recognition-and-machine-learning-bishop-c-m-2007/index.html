<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C. M. 2007) | Louis Tiao</title>
<link href="../../assets/css/all.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/override_nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/">
<link rel="icon" href="../../favicon_16x16.ico" sizes="16x16">
<link rel="icon" href="../../favicon_32x32.ico" sizes="32x32">
<link rel="icon" href="../../favicon_256x256.ico" sizes="256x256">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><!--

    /\\\\\\\\\\\\\\\  /\\\\\\\\\\\     /\\\\\\\\\          /\\\\\              
    \///////\\\/////  \/////\\\///    /\\\\\\\\\\\\\      /\\\///\\\           
           \/\\\           \/\\\      /\\\/////////\\\   /\\\/  \///\\\        
            \/\\\           \/\\\     \/\\\       \/\\\  /\\\      \//\\\      
             \/\\\           \/\\\     \/\\\\\\\\\\\\\\\ \/\\\       \/\\\     
              \/\\\           \/\\\     \/\\\/////////\\\ \//\\\      /\\\     
               \/\\\           \/\\\     \/\\\       \/\\\  \///\\\  /\\\      
                \/\\\        /\\\\\\\\\\\ \/\\\       \/\\\    \///\\\\\/      
                 \///        \///////////  \///        \///       \/////       

--><meta name="author" content="Louis Tiao">
<meta name="robots" content="noindex">
<meta property="og:site_name" content="Louis Tiao">
<meta property="og:title" content="Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C.">
<meta property="og:url" content="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/">
<meta property="og:description" content="Equation 3.13
The gradient of the log-likelihood function is given in equation 3.13:

\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \math">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2015-10-28T16:31:44+11:00">
<meta property="article:tag" content="algebra">
<meta property="article:tag" content="gradients">
<meta property="article:tag" content="linear algebra">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="mathjax">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

  <div class="container">
    <div class="header clearfix">
      <nav><ul class="nav nav-pills pull-right">
<li>
<a href="../../">About</a>
                </li>
<li>
<a href="../../projects/">Projects</a>
                </li>
<li>
<a href="../">Posts</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

          
        </li>
</ul></nav><a href="http://louistiao.me/">

        <h3 class="text-muted">
          <span id="blog-title">Louis Tiao</span>
        </h3>
      </a>
    </div>

<!-- TODO Figure out what to do with this stuff -->
<!--     <div class="row">

      <ul class="nav nav-pills pull-right">
    <li>
    <a href="/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/index.rst" id="sourcelink">Source</a>
    </li>
          
      </ul>
    </div> -->

    <div id="content" role="main">
      <div class="body-content">
        <!--Body content-->
        <div class="row">
          
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C. M. 2007)</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Louis Tiao
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2015-10-28T16:31:44+11:00" itemprop="datePublished" title="2015-10-28 16:31">2015-10-28 16:31</time></a></p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007.html">Comments</a>


                    </p>
<p class="sourceline"><a href="index.rst" class="sourcelink">Source</a></p>

        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="section" id="equation-3-13">
<h2>Equation 3.13</h2>
<p>The gradient of the log-likelihood function is given in equation 3.13:</p>
<div class="math">
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)^T
\end{equation*}
</div>
<p>We derive this step-by-step below. First, the log-likelihood function is given
by equation 3.11 and reproduced below,</p>
<!-- TEASER_END -->
<div class="math">
\begin{align*}
\ln p(\mathbf{t} \mid \mathbf{w}, \beta)
&amp;= \sum_{n=1}^N \ln \mathcal{N}(t_n \mid \mathbf{w}^T \phi(\mathbf{x}_n), \beta^{-1}) \\
&amp;= \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2 \pi - \beta E_D(\mathbf{w})
\end{align*}
</div>
<p>where the sum-of-squares error function is given by equation 3.12,</p>
<div class="math">
\begin{equation*}
E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2
\end{equation*}
</div>
<p>Then <span class="math">\(\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)\)</span>, the
gradient of the log-likelihood function <em>with respect to</em> <span class="math">\(\mathbf{w}\)</span> <a class="footnote-reference" href="#id2" id="id1">[1]</a>
is given by,</p>
<div class="math">
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta) = - \beta \nabla_{\mathbf{w}} E_D(\mathbf{w})
\end{equation*}
</div>
<p>where</p>
<div class="math">
\begin{equation*}
\nabla_{\mathbf{w}} E_D
= \begin{bmatrix}
    \frac{\partial E_D}{\partial w_0} \\
    \vdots \\
    \frac{\partial E_D}{\partial w_{M-1}}
  \end{bmatrix}
\end{equation*}
</div>
<p>and</p>
<div class="math">
\begin{align*}
\frac{\partial E_D}{\partial w_k}
&amp;= \frac{\partial}{\partial w_k} \frac{1}{2} \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 \\
&amp;= \frac{1}{2} \sum_{n=1}^N \frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 \\
\end{align*}
</div>
<p>Since</p>
<div class="math">
\begin{equation*}
\frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2
= 2 \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}
\end{equation*}
</div>
<p>and</p>
<div class="math">
\begin{align*}
\frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}
&amp;= - \frac{\partial}{\partial w_k} \mathbf{w}^T \phi(\mathbf{x}_n) \\
&amp;= - \frac{\partial}{\partial w_k} \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}_n) \\
&amp;= - \phi_k(\mathbf{x}_n)
\end{align*}
</div>
<p>we have</p>
<div class="math">
\begin{equation*}
\frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2
= - 2 \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi_k(\mathbf{x}_n)
\end{equation*}
</div>
<p>and therefore</p>
<div class="math">
\begin{align*}
\frac{\partial E_D}{\partial w_k}
&amp;= \frac{1}{2} \sum_{n=1}^N \frac{\partial}{\partial w_k} \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}^2 \\
&amp;= - \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi_k(\mathbf{x}_n) \\
\end{align*}
</div>
<p>Finally, we have</p>
<div class="math">
\begin{align*}
\nabla_{\mathbf{w}} E_D
&amp;= \begin{bmatrix}
     \frac{\partial E_D}{\partial w_0} \\
     \vdots \\
     \frac{\partial E_D}{\partial w_{M-1}}
   \end{bmatrix} \\
&amp;= - \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \}
   \begin{bmatrix}
     \phi_0(\mathbf{x}_n) \\
     \vdots \\
     \phi_{M-1}(\mathbf{x}_n)
   \end{bmatrix} \\
&amp;= - \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)
\end{align*}
</div>
<p>and</p>
<div class="math">
\begin{equation*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
= \beta \sum_{n=1}^N \{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \} \phi(\mathbf{x}_n)
\end{equation*}
</div>
<p>Note that the final <span class="math">\(\phi(\mathbf{x}_n)\)</span> in equation 3.13 is given as
its transpose, so that <span class="math">\(\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)\)</span>
is a row vector. This was probably done to ease the derivation of the analytic
solution which following immediately in the section. The only thing we need to
alter in our derivation to make the result consistent with that of the book is
to rewrite <span class="math">\(\nabla_{\mathbf{w}} E_D\)</span> as a row vector,</p>
<div class="math">
\begin{equation*}
\nabla_{\mathbf{w}} E_D
= \begin{bmatrix}
    \frac{\partial E_D}{\partial w_0} &amp; \cdots &amp; \frac{\partial E_D}{\partial w_{M-1}}
  \end{bmatrix}
\end{equation*}
</div>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup>
<col class="label">
<col>
</colgroup>
<tbody valign="top"><tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>Since there is little possibility of confusion, the book omits the
subscript and simply writes <span class="math">\(\nabla\)</span> to denote the gradient w.r.t.
<span class="math">\(\mathbf{w}\)</span>.</td>
</tr></tbody>
</table>
</div>
<div class="section" id="equation-3-21">
<h2>Equation 3.21</h2>
<div class="math">
\begin{align*}
\ln p(\mathbf{t} \mid \mathbf{w}, \beta)
&amp;= \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2 \pi - \beta E_D(\mathbf{w}) \\
\frac{\partial}{\partial \beta} \ln p(\mathbf{t} \mid \mathbf{w}, \beta)
&amp;= \frac{N}{2 \beta} - E_D(\mathbf{w})
\end{align*}
</div>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../tags/algebra/" rel="tag">algebra</a></li>
            <li><a class="tag p-category" href="../../tags/gradients/" rel="tag">gradients</a></li>
            <li><a class="tag p-category" href="../../tags/linear-algebra/" rel="tag">linear algebra</a></li>
            <li><a class="tag p-category" href="../../tags/machine-learning/" rel="tag">machine learning</a></li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ltiao",
            disqus_url="http://louistiao.me/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007/",
        disqus_title="Notes: Chapter 3, Pattern Recognition and Machine Learning (Bishop, C. M. 2007)",
        disqus_identifier="cache/posts/notes-chapter-3-pattern-recognition-and-machine-learning-bishop-c-m-2007.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha256-SDRP1VVYu+tgAGKhddBSl5+ezofHKZeI+OzxakbIe/Y=" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true
    },
    displayAlign: 'center', // Change this to 'left' if you want left-aligned equations.
    "HTML-CSS": {
        styles: {'.MathJax_Display': {"margin": 0}}
    }
});
</script></article><script>var disqus_shortname="ltiao";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->
      </div>
    </div>

    <footer id="footer" class="footer">
        
Contents © 2017
<a href="mailto:louistiao@me.com">Louis Tiao</a> - Powered by
<a href="https://getnikola.com" rel="nofollow">Nikola</a>


<span class="pull-right">

  <a class="twitter-follow-button" href="https://twitter.com/louistiao" data-show-count="false" data-show-screen-name="false">
  Follow @louistiao
  </a>

  <a class="github-button" href="https://github.com/ltiao" aria-label="Follow @ltiao on GitHub" data-show-count="false">
  Follow @ltiao
  </a>

  <a href="https://ko-fi.com/A3476EX">
    <object type="image/svg+xml" style="pointer-events: none;" data="https://img.shields.io/badge/Support--yellow.svg?style=social"></object>
  </a>

</span>


            
    </footer>
</div> <!-- /container -->

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><!-- Google Analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-43722566-1', 'auto');
  ga('send', 'pageview');

</script><!-- GitHub Buttons --><script async defer src="https://buttons.github.io/buttons.js"></script><!-- Twitter Widgets --><script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));</script>
</body>
</html>
