{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import (Input, InputLayer, Dense, Lambda, Layer, \n",
    "                          Add, Multiply)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2,\n",
    "                    edgeitems=3,\n",
    "                    linewidth=80,\n",
    "                    suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TensorFlow version: 1.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'TensorFlow version: ' + K.tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [25, 100] \n",
    "mc_sample_sizes = [1, 5, 25]\n",
    "\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Bernoulli negative log likelihood. \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. We require the sum.\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(mc_sample_size, original_dim, latent_dim, intermediate_dim):\n",
    "\n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "    z_mu = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "    z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "    eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                       shape=(K.shape(x)[0],\n",
    "                                              mc_sample_size,\n",
    "                                              latent_dim)))\n",
    "\n",
    "    z_eps = Multiply()([z_sigma, eps])\n",
    "    z = Add()([z_mu, z_eps])\n",
    "\n",
    "    decoder = Sequential([\n",
    "        Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "        Dense(original_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    x_mean = decoder(z)\n",
    "\n",
    "    return Model(inputs=[x, eps], outputs=x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, original_dim) / 255.\n",
    "x_test = x_test.reshape(-1, original_dim) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 25 | MC sample size 1\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 8s 128us/step - loss: 175.1970 - val_loss: 166.8884\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 164.7072 - val_loss: 162.9183\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 161.5457 - val_loss: 160.4000\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 159.7933 - val_loss: 159.0190\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 158.6401 - val_loss: 158.6160\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 157.8559 - val_loss: 157.5280\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 157.1781 - val_loss: 156.8357\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 156.6294 - val_loss: 156.9744\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 156.1520 - val_loss: 156.3271\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 155.7995 - val_loss: 155.6093\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 155.4761 - val_loss: 155.8117\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 155.2142 - val_loss: 155.3306\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 154.9656 - val_loss: 155.0260\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 154.8004 - val_loss: 155.0653\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 154.5534 - val_loss: 155.1766\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 154.3994 - val_loss: 154.8735\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 154.2507 - val_loss: 154.5887\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 154.0771 - val_loss: 154.5812\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 153.9040 - val_loss: 154.7645\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 153.7820 - val_loss: 153.9356\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 153.6550 - val_loss: 153.9721\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 153.5831 - val_loss: 153.7020\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 153.4567 - val_loss: 153.6845\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 153.3308 - val_loss: 153.8533\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 153.3213 - val_loss: 153.9720\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 153.2821 - val_loss: 154.1556\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 153.2045 - val_loss: 154.0935\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 153.1441 - val_loss: 154.0715\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 153.0656 - val_loss: 154.3373\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 153.0043 - val_loss: 153.9538\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 152.9843 - val_loss: 153.5975\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 152.9990 - val_loss: 153.8054\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 152.9434 - val_loss: 153.4264\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 152.9533 - val_loss: 153.5024\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 153.0167 - val_loss: 153.8569\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 152.9412 - val_loss: 153.9134\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 153.0292 - val_loss: 154.2639\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 153.0803 - val_loss: 154.0290\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 153.1670 - val_loss: 154.3663\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 153.2082 - val_loss: 154.5731\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 153.5592 - val_loss: 155.5790\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 153.8169 - val_loss: 156.9126\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 154.1240 - val_loss: 158.0237\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 154.5585 - val_loss: 156.4545\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 155.3278 - val_loss: 157.5434\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 158.5592 - val_loss: 169.0280\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 1593.4144 - val_loss: 2376.2710\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 7s 117us/step - loss: 14415.9808 - val_loss: 721.2827\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 1791.8892 - val_loss: 148322.9120\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 4522.4146 - val_loss: 347087.0158\n",
      "batch size 25 | MC sample size 5\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 175.3207 - val_loss: 166.8680\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 164.8280 - val_loss: 163.2099\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 161.6924 - val_loss: 160.6036\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 159.4306 - val_loss: 158.7564\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 158.0676 - val_loss: 158.0571\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 157.1136 - val_loss: 156.9112\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 156.3888 - val_loss: 156.2834\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 155.8152 - val_loss: 155.7561\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 155.3040 - val_loss: 155.4674\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 154.8682 - val_loss: 155.2643\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 154.4962 - val_loss: 154.6515\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 154.1617 - val_loss: 154.5908\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 153.8709 - val_loss: 154.3303\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 153.6015 - val_loss: 154.8278\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 153.3589 - val_loss: 153.8198\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 153.1319 - val_loss: 153.6191\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 152.9374 - val_loss: 153.9826\n",
      "Epoch 18/50\n",
      " 3500/60000 [>.............................] - ETA: 6s - loss: 151.8664"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "\n",
    "    histories_batch_size = []\n",
    "    \n",
    "    for mc_sample_size in mc_sample_sizes:\n",
    "\n",
    "        print('batch size {} | MC sample size {}'\n",
    "              .format(batch_size, mc_sample_size))\n",
    "\n",
    "        x_train_target = np.tile(np.expand_dims(x_train, axis=1),\n",
    "                                 reps=(1, mc_sample_size, 1))\n",
    "        x_test_target = np.tile(np.expand_dims(x_test, axis=1),\n",
    "                                reps=(1, mc_sample_size, 1))\n",
    "\n",
    "        vae = build_vae(mc_sample_size, original_dim, latent_dim, \n",
    "                        intermediate_dim)\n",
    "        vae.compile(optimizer='rmsprop', loss=nll)\n",
    "\n",
    "        histories_batch_size.append(\n",
    "            vae.fit(x_train,\n",
    "                    x_train_target,\n",
    "                    shuffle=True,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, x_test_target))\n",
    "        )\n",
    "    \n",
    "    histories.append(histories_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_figsize = lambda width: (width, 2. * width / (1 + np.sqrt(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=golden_figsize(6))\n",
    "\n",
    "# for batch_size in batch_sizes:\n",
    "for mc_sample_size in mc_sample_sizes:\n",
    "    \n",
    "    pd.DataFrame(histories[25][mc_sample_size].history) \\\n",
    "    .plot(y='loss', label='MC samples: {:2d}'.format(mc_sample_size), ax=ax)\n",
    "\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.set_xlabel('# epochs')\n",
    "\n",
    "ax.set_ylim(145, 170)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "category": "",
   "date": "2017-07-21 18:38:07 UTC+10:00",
   "description": "",
   "link": "",
   "slug": "variational-inference-with-implicit-approximate-inference-models-wip-pt-8",
   "tags": "",
   "title": "Variational Inference with Implicit Approximate Inference Models (WIP Pt. 8)",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
