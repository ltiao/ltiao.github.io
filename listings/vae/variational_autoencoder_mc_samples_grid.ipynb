{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import (Input, InputLayer, Dense, Lambda, Layer, \n",
    "                          Add, Multiply)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2,\n",
    "                    edgeitems=3,\n",
    "                    linewidth=80,\n",
    "                    suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TensorFlow version: 1.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'TensorFlow version: ' + K.tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constant definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_sample_sizes = [1, 5, 25]\n",
    "\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "epochs = 50\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Bernoulli negative log likelihood. \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. We require the sum.\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vae(mc_sample_size, original_dim, latent_dim, intermediate_dim):\n",
    "\n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "    z_mu = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "    z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "    eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                       shape=(K.shape(x)[0],\n",
    "                                              mc_sample_size,\n",
    "                                              latent_dim)))\n",
    "\n",
    "    z_eps = Multiply()([z_sigma, eps])\n",
    "    z = Add()([z_mu, z_eps])\n",
    "\n",
    "    decoder = Sequential([\n",
    "        Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "        Dense(original_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    x_mean = decoder(z)\n",
    "\n",
    "    return Model(inputs=[x, eps], outputs=x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, original_dim) / 255.\n",
    "x_test = x_test.reshape(-1, original_dim) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_history(batch_size):\n",
    "\n",
    "    histories = []\n",
    "    \n",
    "    for mc_sample_size in mc_sample_sizes:\n",
    "\n",
    "        print('batch size {} | MC sample size {}'\n",
    "              .format(batch_size, mc_sample_size))\n",
    "\n",
    "        x_train_target = np.tile(np.expand_dims(x_train, axis=1),\n",
    "                                 reps=(1, mc_sample_size, 1))\n",
    "        x_test_target = np.tile(np.expand_dims(x_test, axis=1),\n",
    "                                reps=(1, mc_sample_size, 1))\n",
    "\n",
    "        vae = build_vae(mc_sample_size, original_dim, latent_dim, \n",
    "                        intermediate_dim)\n",
    "        vae.compile(optimizer='rmsprop', loss=nll)\n",
    "\n",
    "        histories.append(\n",
    "            vae.fit(x_train,\n",
    "                    x_train_target,\n",
    "                    shuffle=True,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_test, x_test_target))\n",
    "        )\n",
    "        \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_figsize = lambda width: (width, 2. * width / (1 + np.sqrt(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 50 | MC sample size 1\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 180.6965 - val_loss: 169.0988\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 167.0343 - val_loss: 165.1639\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 163.6653 - val_loss: 162.4237\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 161.1999 - val_loss: 160.4553\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 159.2697 - val_loss: 158.9443\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 157.9768 - val_loss: 158.0256\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 157.0703 - val_loss: 157.0299\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 156.3371 - val_loss: 156.5523\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 155.7688 - val_loss: 156.0917\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 155.2029 - val_loss: 155.4437\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 154.7253 - val_loss: 155.1678\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 154.3346 - val_loss: 154.9393\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 153.9583 - val_loss: 154.4159\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 153.6195 - val_loss: 154.1213\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 153.3176 - val_loss: 154.1436\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 153.0259 - val_loss: 153.6267\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 152.7615 - val_loss: 153.5734\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 152.5093 - val_loss: 153.2711\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 152.2701 - val_loss: 153.2701\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 152.0587 - val_loss: 153.1165\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 151.8802 - val_loss: 153.3086\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 151.6620 - val_loss: 152.7274\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 151.4945 - val_loss: 152.5977\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 151.3292 - val_loss: 152.5565\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 151.1882 - val_loss: 152.2743\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 151.0286 - val_loss: 152.7742\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 150.8754 - val_loss: 152.4209\n",
      "Epoch 28/50\n",
      " 5450/60000 [=>............................] - ETA: 3s - loss: 150.2125"
     ]
    }
   ],
   "source": [
    "histories = fit_history(batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=golden_figsize(6))\n",
    "\n",
    "# for batch_size in batch_sizes:\n",
    "for i in range(len(mc_sample_sizes)):\n",
    "\n",
    "    pd.DataFrame(histories[0][i].history) \\\n",
    "    .plot(y='loss', label='MC samples: {:2d}'.format(mc_sample_sizes[i]), ax=ax)\n",
    "\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.set_xlabel('# epochs')\n",
    "\n",
    "ax.set_ylim(145, 170)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nikola": {
   "category": "",
   "date": "2017-07-21 18:38:07 UTC+10:00",
   "description": "",
   "link": "",
   "slug": "variational-inference-with-implicit-approximate-inference-models-wip-pt-8",
   "tags": "",
   "title": "Variational Inference with Implicit Approximate Inference Models (WIP Pt. 8)",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
